import requests
import json
import time
import sys
from bs4 import BeautifulSoup

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

# ç¢ºä¿å¼•ç”¨ä½ çš„åœ°é»æ¸…å–®
from scraper_final import all_locations

def scrape_description(pid, location_name):
    # ğŸ’¡ ä¿®æ­£å¾Œçš„æ­£ç¢ºç¶²å€ï¼šå»æ‰ _PC
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/Detail/{pid}_Detail.html"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.cwa.gov.tw/V8/C/L/StarView/StarView.html"
    }

    try:
        # åŠ å…¥ timestamp åƒæ•¸æ¨¡æ“¬ç€è¦½å™¨è¡Œç‚º (é›–éå¿…è¦ä½†è¼ƒä¿éšª)
        params = {"T": int(time.time())}
        response = requests.get(url, headers=headers, params=params, timeout=10)
        
        if response.status_code != 200:
            print(f" âš ï¸ {location_name} ({pid}) è«‹æ±‚å¤±æ•— (Code: {response.status_code})")
            return "ç›®å‰æš«ç„¡è©³ç´°ä»‹ç´¹ã€‚"
            
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        
        # å°‹æ‰¾ç°¡ä»‹å€å¡Š (æ°£è±¡å±€æœ‰æ™‚å€™ç”¨ Detail_MODï¼Œæœ‰æ™‚å€™å¯èƒ½çµæ§‹å¾®èª¿ï¼Œé€™è£¡é€šæŠ“)
        # å…ˆå˜—è©¦æŠ“ id="Detail_MOD"
        detail_div = soup.find("div", id="Detail_MOD")
        
        # å¦‚æœ id æŠ“ä¸åˆ°ï¼Œå˜—è©¦ç›´æ¥æŠ“å…§æ–‡çš„ p (é˜²å‘†æ©Ÿåˆ¶)
        if not detail_div:
            detail_div = soup
            
        p_tags = detail_div.find_all("p")
        if p_tags:
            # åˆä½µå¤šæ®µæ–‡å­—ï¼Œå»é™¤å¤šé¤˜ç©ºç™½
            desc = "\n".join([p.get_text(strip=True) for p in p_tags if p.get_text(strip=True)])
            if len(desc) > 5: # ç¢ºä¿æŠ“åˆ°çš„ä¸æ˜¯ç©ºå­—ä¸²
                return desc
        
        return "ç›®å‰æš«ç„¡è©³ç´°ä»‹ç´¹ã€‚"
    except Exception as e:
        print(f" âŒ {location_name} ç™¼ç”ŸéŒ¯èª¤: {e}")
        return "è³‡æ–™è®€å–å¤±æ•—ã€‚"

if __name__ == "__main__":
    print("ğŸš€ æœ€çµ‚å˜—è©¦ï¼šçˆ¬å–å…¨å°æ™¯é»ç°¡ä»‹ (ä¿®æ­£æª”å)...")
    descriptions = {}
    
    for pid, name in all_locations.items():
        desc = scrape_description(pid, name)
        descriptions[pid] = desc
        # å°å‡ºå‰ 10 å€‹å­—æª¢æŸ¥æœ‰æ²’æœ‰æŠ“å°
        print(f" âœ… {name}: {desc[:10]}...") 
        time.sleep(0.5)
        
    with open("spot_descriptions.json", "w", encoding="utf-8") as f:
        json.dump(descriptions, f, ensure_ascii=False, indent=4)
        
    print(f"\nğŸ‰ ç°¡ä»‹æŠ“å–å®Œæˆï¼è«‹æª¢æŸ¥ spot_descriptions.json")