import requests
import pandas as pd
import time
import sys
import re  # âœ… è¨˜å¾—å¼•å…¥ re å¥—ä»¶ä¾†è™•ç†æ–‡å­—
from bs4 import BeautifulSoup

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- 1. ä½ çš„æ‰‹å‹•åœ°é»æ¸…å–® ---
all_locations = {
    "F010": "åŸºéš†å¤§æ­¦å´™ç ²å°åœè»Šå ´",
    "F022": "é™½æ˜å±±åœ‹å®¶å…¬åœ’å°æ²¹å‘åœè»Šå ´",
    "F023": "é™½æ˜å±±åœ‹å®¶å…¬åœ’æ“å¤©å´—",
    "F011": "äº”åˆ†å±±",
    "F012": "çŸ³ç¢‡é›²æµ·åœ‹å°",
    "F013": "çƒä¾†é¢¨æ™¯ç‰¹å®šå€",
    "F014": "è§€éœ§æ£®æ—éŠæ¨‚å€",
    "F019": "å¤§é›ªå±±åœ‹å®¶æ£®æ—éŠæ¨‚å€",
    "F018": "æ­¦é™µè¾²å ´",
    "F020": "ç¦å£½å±±è¾²å ´",
    "F021": "è‡ºä¸­éƒ½æœƒå…¬åœ’",
    "F002": "å°é¢¨å£åœè»Šå ´",
    "F016": "æ–°ä¸­æ©«å¡”å¡”åŠ åœè»Šå ´",
    "F004": "è‡ºå¤§å±±åœ°å¯¦é©—è¾²å ´",
    "F003": "é³¶å³°åœè»Šå ´",
    "F015": "é˜¿é‡Œå±±éŠæ¨‚å€",
    "F017": "é¹¿æ—å¤©æ–‡å°",
    "F024": "ä¸ƒè‚¡æµ·å ¤",
    "F025": "å—ç€›å¤©æ–‡é¤¨",
    "F026": "è‡ºå—éƒ½æœƒå…¬åœ’",
    "F007": "é«˜é›„æ¢…å±±é’å¹´æ´»å‹•ä¸­å¿ƒ",
    "F009": "é«˜é›„éƒ½æœƒå…¬åœ’",
    "F008": "è—¤ææ£®æ—éŠæ¨‚å€",
    "F005": "å¢¾ä¸è²“é¼»é ­",
    "F006": "å¢¾ä¸é¾ç£å…¬åœ’",
    "F001": "å¤ªå¹³å±±æ£®æ—éŠæ¨‚å€",
}

# --- 2. æ”¹è‰¯ç‰ˆçˆ¬èŸ²å‡½å¼ (å«ç´«å¤–ç·šæ ¼å¼ä¿®æ­£) ---
def scrape_weekly_table(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/Week/{pid}_Week_PC.html"
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200: return []
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        
        # 1. è™•ç†è¡¨é ­ (æ—¥æœŸ)
        thead = soup.find("thead")
        if not thead: return []
        dates = []
        date_row = thead.find_all("tr")[0].find_all("th")
        for th in date_row:
            text = th.get_text(strip=True)
            if not text or text == "æ—¥æœŸ": continue
            for _ in range(int(th.get('colspan', 1))): dates.append(text)
            
        # 2. è™•ç†å…§å®¹ (æ•¸æ“š)
        tbody = soup.find("tbody")
        if not tbody: return []
        parsed_data = {}
        
        for row in tbody.find_all("tr"):
            th = row.find("th")
            if not th: continue
            row_name = th.get_text(strip=True)
            
            vals = []
            for td in row.find_all("td"):
                # --- A. å–å€¼é‚è¼¯ ---
                img = td.find("img")
                tem_c = td.find("span", class_="tem-C") 
                
                if img:
                    val = img.get('title') or img.get('alt')
                elif tem_c:
                    val = tem_c.get_text(strip=True)
                else:
                    val = td.get_text(strip=True)
                
                # --- B. åˆ¤æ–·æ˜¯å¦ç‚ºã€Œç„¡è³‡æ–™ã€ ---
                if val == "-" or val == "" or val is None:
                    val = "æœªçŸ¥"

                # --- C. âœ… æ–°å¢ï¼šç´«å¤–ç·šæ ¼å¼ä¿®æ­£ ---
                # åŸå§‹è³‡æ–™å¯èƒ½æ˜¯ "2ä½é‡ç´š"ï¼Œæˆ‘å€‘è¦æ”¹æˆ "ä½é‡ç´š(æŒ‡æ•¸2)"
                if "ç´«å¤–ç·š" in row_name and val != "æœªçŸ¥":
                    # ä½¿ç”¨ Regex åˆ†é›¢æ•¸å­—å’Œæ–‡å­—
                    # (\d+) æŠ“æ•¸å­—ï¼Œ (.*) æŠ“å‰©ä¸‹çš„æ–‡å­—
                    match = re.match(r"^(\d+)(.*)$", val)
                    if match:
                        num = match.group(1)   # ä¾‹å¦‚ "2"
                        desc = match.group(2)  # ä¾‹å¦‚ "ä½é‡ç´š"
                        val = f"{desc}(æŒ‡æ•¸{num})" # çµ„åˆæˆ "ä½é‡ç´š(æŒ‡æ•¸2)"
                
                vals.append(val)
            
            parsed_data[row_name] = vals
            
        # 3. è½‰ç½®è³‡æ–™
        results = []
        for i in range(len(dates)):
            item = {
                "location": location_name, 
                "pid": pid, 
                "date": dates[i], 
                "time_desc": "ç™½å¤©" if i%2==0 else "æ™šä¸Š"
            }
            for k, v in parsed_data.items(): 
                item[k] = v[i] if i < len(v) else "æœªçŸ¥"
            results.append(item)
            
        return results
    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤: {e}")
        return []

# --- ä¸»ç¨‹å¼ ---
if __name__ == "__main__":
    if not all_locations:
        print("âŒ è«‹æª¢æŸ¥åœ°é»æ¸…å–®æ˜¯å¦ç‚ºç©ºï¼")
    else:
        print(f"ğŸš€ é–‹å§‹çˆ¬å– {len(all_locations)} å€‹åœ°é»...")
        final_data = []
        
        count = 0
        for pid, name in all_locations.items():
            data = scrape_weekly_table(pid, name)
            if data:
                final_data.extend(data)
                count += 1
                print(f"   [{count}/{len(all_locations)}] {name} - å®Œæˆ")
            else:
                print(f"   [{count}/{len(all_locations)}] {name} - ç„¡è³‡æ–™")
            
            time.sleep(0.2) 

        if final_data:
            df = pd.DataFrame(final_data)
            df.to_csv("all_taiwan_star_forecast.csv", index=False, encoding="utf-8-sig")
            print(f"\nâœ… æˆåŠŸï¼å·²å„²å­˜ {len(final_data)} ç­†è³‡æ–™")
            print("âœ¨ ç´«å¤–ç·šæ ¼å¼å·²çµ±ä¸€ä¿®æ­£ç‚º 'ç­‰ç´š(æ•¸å€¼)'")