import pandas as pd
import time
import sys
import os
import re
from datetime import datetime, timedelta, timezone
from curl_cffi import requests as cffi_requests # âœ¨ é—œéµï¼šä½¿ç”¨å½è£ç€è¦½å™¨è«‹æ±‚

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

# ==========================================
# ğŸ”‘ è¨­å®šå€
# ==========================================
CWA_API_KEY = os.getenv("CWA_API_KEY") 

# ==========================================
# ğŸ“ åœ°é»æ¸…å–® (å°æ‡‰ API ID)
# ==========================================
all_locations = {
    "F001": "å¤ªå¹³å±±æ£®æ—éŠæ¨‚å€", "F002": "å°é¢¨å£åœè»Šå ´", "F003": "é³¶å³°åœè»Šå ´",
    "F004": "å°å¤§æ¢…å³°å¯¦é©—è¾²å ´", "F005": "å¢¾ä¸è²“é¼»é ­", "F006": "å¢¾ä¸é¾ç£å…¬åœ’",
    "F007": "é«˜é›„æ¢…å±±é’å¹´æ´»å‹•ä¸­å¿ƒ", "F008": "è—¤ææ£®æ—éŠæ¨‚å€", "F009": "é«˜é›„éƒ½æœƒå…¬åœ’",
    "F010": "åŸºéš†å¤§æ­¦å´™ç ²å°åœè»Šå ´", "F011": "äº”åˆ†å±±", "F012": "çŸ³ç¢‡é›²æµ·åœ‹å°",
    "F013": "çƒä¾†é¢¨æ™¯ç‰¹å®šå€", "F014": "è§€éœ§æ£®æ—éŠæ¨‚å€", "F015": "é˜¿é‡Œå±±éŠæ¨‚å€",
    "F016": "æ–°ä¸­æ©«å¡”å¡”åŠ åœè»Šå ´", "F017": "é¹¿æ—å¤©æ–‡å°", "F018": "æ­¦é™µè¾²å ´",
    "F019": "å¤§é›ªå±±åœ‹å®¶æ£®æ—éŠæ¨‚å€", "F020": "ç¦å£½å±±è¾²å ´", "F021": "è‡ºä¸­éƒ½æœƒå…¬åœ’",
    "F022": "é™½æ˜å±±åœ‹å®¶å…¬åœ’å°æ²¹å‘åœè»Šå ´", "F023": "é™½æ˜å±±åœ‹å®¶å…¬åœ’æ“å¤©å´—",
    "F024": "ä¸ƒè‚¡æµ·å ¤", "F025": "å—ç€›å¤©æ–‡æ•™è‚²åœ’å€", "F026": "è‡ºå—éƒ½æœƒå…¬åœ’"
}

# ==========================================
# ğŸ› ï¸ æ ¸å¿ƒå‡½å¼ï¼šä½¿ç”¨ curl_cffi ä¸‹è¼‰ API (æŠ—å°é–ç‰ˆ)
# ==========================================
def fetch_file_api_data(data_id):
    """
    ä½¿ç”¨ curl_cffi æ¨¡æ“¬ Chrome ç€è¦½å™¨ä¸‹è¼‰æ°£è±¡ç½² APIï¼Œ
    å¾¹åº•è§£æ±º SSL æ†‘è­‰éŒ¯èª¤ (Missing Subject Key Identifier) èˆ‡ WAF é˜»æ“‹å•é¡Œã€‚
    """
    url = f"https://opendata.cwa.gov.tw/fileapi/v1/opendataapi/{data_id}"
    params = {"Authorization": CWA_API_KEY, "format": "JSON"} # ç”¨æˆ‘çš„æ†‘è­‰ä¸‹è¼‰jsnæª”
    
    try:
        # âœ¨ é­”æ³•åœ¨é€™è£¡ï¼šimpersonate="chrome110"
        # Google Chrome å¤§ç´„æ¯å€‹æœˆéƒ½æœƒæ›´æ–°ä¸€æ¬¡ç‰ˆæœ¬è™Ÿï¼ˆä¾‹å¦‚ Chrome 108, 109, 110... ç›´åˆ°ç¾åœ¨çš„ 120+ï¼‰ã€‚
        # chrome110 æ˜¯ curl_cffi å‡½å¼åº«ä¸­ä¸€å€‹éå¸¸ç©©å®šä¸”å…·å‚™ç¾ä»£å®‰å…¨ç‰¹å¾µï¼ˆå¦‚ HTTP/2ï¼‰çš„é è¨­å½è£ç›®æ¨™ã€‚
        response = cffi_requests.get(
            url, 
            params=params, # æŠŠåƒæ•¸è‡ªå‹•åŠ åœ¨url(ç¶²å€)å¾Œé¢
            impersonate="chrome110", 
            timeout=30
        )
        
        if response.status_code == 200:
            return response.json()
        else:
            # æˆåŠŸé€£ç·šä½†æ²’æ‹¿åˆ°è³‡æ–™(è¢«å°é–æˆ–æ˜¯æŸ¥ä¸åˆ°)
            print(f"âŒ API ä¸‹è¼‰å¤±æ•— (Status {response.status_code}): {url}")
            return None
    except Exception as e:
        # é€£ç·šå¤±æ•—
        print(f"âŒ API é€£ç·šåš´é‡éŒ¯èª¤: {e}")
        return None

def find_location_data(json_data, target_pid):
    try:
        root = json_data.get('cwaopendata', {})
        dataset = root.get('Dataset', root.get('dataset', {})) 
        locations_node = dataset.get('Locations', dataset.get('locations', {}))
        location_list = locations_node.get('Location', locations_node.get('location', [])) 
        # getåŒ…è‘—get:å…ˆåŸ·è¡Œå‰é¢ï¼Œç„¡è³‡æ–™å†åŸ·è¡Œå¾Œé¢ï¼Œä¾‹å¦‚å…ˆæ‰¾å¤§å°Dï¼Œå†æ‰¾å°å¯«dï¼Œå¦‚æœéƒ½æ²’æœ‰å°±å›å‚³è‡ªè¨‚çš„ç©ºçš„é è¨­å‹æ…‹(å­—å…¸æˆ–ä¸²åˆ—)ã€‚
        # æœ€å¾Œæ‰¾åˆ°åŒ…å«ä¸åŒåœ°é»(å­—å…¸å‹æ…‹)çš„è³‡è¨Šä¸¦åŒ…æˆä¸²åˆ—ã€‚

        for loc in location_list:
            current_pid = None
            param_set = loc.get('ParameterSet', loc.get('parameterSet', {}))
            params = param_set.get('Parameter', param_set.get('parameter', []))
            
            # æª¢æŸ¥ params æ˜¯ä¸æ˜¯æ¸…å–®ã€‚å¦‚æœæ˜¯ï¼Œå°±ç›´æ¥ç”¨ï¼›
            # å¦‚æœæ˜¯å­—å…¸æˆ–å…¶ä»–ï¼Œå°±æŠŠå®ƒåŒ…é€²ä¸­æ‹¬è™Ÿ [] è£¡ã€‚é€™æ¨£å¾Œé¢çš„ for p in p_list æ‰èƒ½çµ±ä¸€é‹ä½œã€‚
            p_list = params if isinstance(params, list) else [params]
            for p in p_list:
                if p.get('ParameterName') == 'id':
                    current_pid = p.get('ParameterValue')
                    break
            
            # target_pidæ˜¯å¾all_locations.items()å‡½å¼è·³è½‰éä¾†çš„ã€‚
            # all_locations.items()åŸ·è¡Œè¿´åœˆï¼Œå†å‘¼å«find_location_data()ã€‚
            if current_pid == target_pid:
                return loc
        return None
    except: return None

# ==========================================
# ğŸ’¾ CSV æ›´æ–°åŠŸèƒ½ (ä¿ç•™ä½ çš„æ­·å²è³‡æ–™éœ€æ±‚)
# ==========================================
def update_weekly_csv():
    # æŠ“å–ã€Œæœªä¾†ä¸€é€± (F-B0053-069)ã€è³‡æ–™
    data = fetch_file_api_data("F-B0053-069")
    if not data:
        print("âŒ ç„¡æ³•å–å¾—ä¸€é€±é å ±è³‡æ–™ï¼Œè·³é CSV æ›´æ–°ã€‚")
        return

    csv_data = [] # æŠ“å–å®Œæˆä¸¦éœ€è¦å„²å­˜çš„è³‡æ–™
    print(f"ğŸš€ æ­£åœ¨æ›´æ–° CSV è³‡æ–™åº«...")

    for pid, name in all_locations.items():
        loc = find_location_data(data, pid)
        if not loc: continue
        
        raw_elements = loc.get('WeatherElement', [])
        elements = {item.get('ElementName'): item.get('Time', []) for item in raw_elements}

        # F-B0053-069 æ¯ 12 å°æ™‚ä¸€ç­† (æ—©/æ™š)
        ref_time_list = elements.get('å¤©æ°£ç¾è±¡', [])
        
        # ç”¨ enumerate è™•ç†ã€Œå¤šå€‹ä¸¦åˆ—æ¸…å–®ã€çš„åŒæ­¥å•é¡Œã€‚
        # ç•¶æˆ‘å€‘ç”¨ enumerate è·‘ã€Œå¤©æ°£ç¾è±¡ã€æ¸…å–®æ™‚ï¼Œæˆ‘å€‘æ‹¿åˆ°äº†ç·¨è™Ÿ iã€‚
        # é€™æ™‚æˆ‘å€‘å°±å¯ä»¥ç”¨åŒä¸€å€‹ç·¨è™Ÿ i å»ã€Œæœ€ä½æº«åº¦ã€æ¸…å–®è£¡æŠ“å‡ºå°æ‡‰çš„æº«åº¦ã€‚
        for i, time_item in enumerate(ref_time_list):
            start_str = time_item.get('StartTime') 
            if not start_str: continue
            
            # æŠŠå¤©æ°£ç¾è±¡çš„é–‹å§‹æ™‚é–“è½‰æ›æˆ"01/06"æ ¼å¼
            # å…ˆå»ºç«‹å¯¦ä¾‹å±¬æ€§ï¼Œå†é€éæ–¹æ³•è½‰æˆæ–°çš„æ ¼å¼
            # ä¸€é€±æ—¥å¤œè³‡æ–™æ˜¯ä»¥6åˆ°18æ™‚åšåˆ‡å‰²
            dt = datetime.fromisoformat(start_str)
            date_display = dt.strftime("%m/%d")
            time_label = "æ™šä¸Š" if dt.hour == 18 else "ç™½å¤©"

            try:
                # time_itemç¾åœ¨æ˜¯ä¸€å€‹å·¢ç‹€å­—å…¸ï¼Œå–å‡ºå¾Œï¼Œwxæ˜¯å¤©æ°£ç‹€æ³
                wx = time_item['ElementValue']['Weather']
                
                # è¼”åŠ©å‡½å¼ï¼šå®‰å…¨å–å¾—æ•¸å€¼
                def get_val(key, idx):
                    # å¾ elements å­—å…¸ï¼ˆè£¡é¢å­˜äº†æ‰€æœ‰å¤©æ°£å…ƒç´ ï¼‰ä¸­ï¼Œ
                    # æ ¹æ“š keyï¼ˆä¾‹å¦‚ï¼š'æœ€ä½æº«åº¦'ï¼‰æŠŠé‚£ä¸€é•·ä¸²çš„æ™‚é–“æ¸…å–®æ‹¿å‡ºä¾†ã€‚
                    lst = elements.get(key, [])
                    # ã€Œé‚Šç•Œæª¢æŸ¥ã€ã€‚è¬ä¸€ã€Œå¤©æ°£ç¾è±¡ã€æœ‰ 7 ç­†é å ±ï¼Œä½†ã€Œé™é›¨æ©Ÿç‡ã€åªæœ‰ 5 ç­†ï¼Œ
                    # å¦‚æœä¸æª¢æŸ¥ï¼Œç¨‹å¼è·‘åˆ°ç¬¬ 6 ç­†æ™‚å°±æœƒå› ç‚ºæ‰¾ä¸åˆ°è³‡æ–™è€Œç›´æ¥ç•¶æ©Ÿã€‚
                    # è§£æ³•ï¼šval_dict.values() æœƒæŠŠè£¡é¢æ‰€æœ‰çš„ã€Œå€¼ã€éƒ½æ‹¿å‡ºä¾†ï¼ˆä¸çœ‹ Keyï¼‰ï¼Œç„¶å¾Œè½‰æˆ list æ‹¿ç¬¬ä¸€å€‹ [0]ã€‚
                    # é€™æ¨£ä¸ç®¡æ˜¯å¤©æ°£é‚„æ˜¯æº«åº¦ï¼Œéƒ½èƒ½æ­£ç¢ºæŠ“åˆ°é‚£å€‹ã€Œæ•¸å€¼ã€ã€‚
                    if idx < len(lst):
                        val_dict = lst[idx]['ElementValue']
                        return list(val_dict.values())[0] # å–ç¬¬ä¸€å€‹å€¼æœ€ä¿éšª
                    return "-"

                min_t = get_val('æœ€ä½æº«åº¦', i)
                max_t = get_val('æœ€é«˜æº«åº¦', i)
                min_at = get_val('æœ€ä½é«”æ„Ÿæº«åº¦', i)
                max_at = get_val('æœ€é«˜é«”æ„Ÿæº«åº¦', i)
                pop = get_val('12å°æ™‚é™é›¨æ©Ÿç‡', i)
                pop = f"{pop}%" if pop != " " else "0%"
                wind = get_val('è’²ç¦é¢¨ç´š', i)

                row = {
                    "location": name, "pid": pid, "date": date_display, "æ™‚é–“": time_label,
                    "å¤©æ°£ç‹€æ³": wx, "æœ€ä½æº«": min_t, "æœ€é«˜æº«": max_t,
                    "é«”æ„Ÿæœ€ä½æº«": min_at, "é«”æ„Ÿæœ€é«˜æº«": max_at,
                    "é™é›¨æ©Ÿç‡": pop, "è’²ç¦é¢¨ç´š": wind
                }
                csv_data.append(row)
            except: continue

    if csv_data:
        file_name = "all_taiwan_star_forecast.csv"
        df = pd.DataFrame(csv_data)
        df.to_csv(file_name, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV æ›´æ–°æˆåŠŸï¼å·²å¯«å…¥ {len(df)} ç­†è³‡æ–™ã€‚")
    else:
        print("âš ï¸ é›–ç„¶æŠ“åˆ° API ä½†æ²’æœ‰è§£æå‡ºæœ‰æ•ˆè³‡æ–™ã€‚")

# ==========================================
# ğŸ”­ åŠŸèƒ½ Aï¼šä»Šæ™šè§€æ˜Ÿ (ä½¿ç”¨ F-B0053-071)
# ==========================================
def format_time_ranges(time_list):
    if not time_list: return ""
    hours = []
    for t in time_list:
        try: hours.append(int(t.split(':')[0]))
        except: continue
    if not hours: return ""

    # åˆ¤æ–·æ˜¯å¦æœ‰ã€Œæ™šä¸Šï¼ˆ18é»ä»¥å¾Œï¼‰ã€çš„è³‡æ–™
    # å¦‚æœæ™‚é–“æ¸…å–®è£¡åŒæ™‚æœ‰ ã€Œæ™šä¸Šï¼ˆ>=18ï¼‰ã€ å’Œ ã€Œå‡Œæ™¨ï¼ˆ<=5ï¼‰ã€ï¼Œä»£è¡¨é€™æ˜¯è·¨å¤œçš„æƒ…æ³ã€‚
    # è½‰æ›ä¸¦æ’åºå¾Œ processedæ ¼å¼: [23, 24, 25, 28, 29]
    has_evening = any(h >= 18 for h in hours)
    processed = [h + 24 if (h <= 5 and has_evening) else h for h in hours]
    processed.sort()
    
    ranges = []
    if not processed: return ""
    # åˆå§‹åŒ–æŒ‡æ¨™ï¼šè¨˜ä½ã€Œé€™ä¸€çµ„ã€çš„èµ·é» (start_h) å’Œ ä¸Šä¸€å€‹è™•ç†çš„æ•¸å­— (prev_h)
    start_h = prev_h = processed[0]
    
    for i in range(1, len(processed)): # å¾ç¬¬2å€‹é–‹å§‹
        curr = processed[i]
        # åˆ¤æ–·é€£çºŒï¼šå¦‚æœã€Œç¾åœ¨é€™å€‹æ•¸å­—ã€ç­‰æ–¼ã€Œä¸Šå€‹æ•¸å­— + 1ã€
        # å°±æŠŠã€Œå‰ä¸€å€‹ã€å¾€å¾Œæ¨ï¼Œç¹¼çºŒä¸²ä¸‹å»
        # å¦‚æœæ–·æ‰äº†(ä¾‹å¦‚å¾ 25 è·³åˆ° 28)ï¼Œå…ˆæŠŠå‰é¢é‚£ä¸€çµ„ (23~25) çµç®—ä¸¦å­˜èµ·ä¾†
        # é–‹å•Ÿæ–°çš„ä¸€çµ„ï¼Œèµ·é»è¨­ç‚ºç¾åœ¨é€™å€‹æ•¸å­— (28)
        # ç”¨é¤˜æ•¸å°‡åŠ 24å°æ™‚çš„å‡Œæ™¨æ™‚é–“(0-5)è®Šå›æ¨™æº–æ™‚é–“
        if curr == prev_h + 1: prev_h = curr
        else:
            ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
            start_h = prev_h = curr
    ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
    return "ã€".join(ranges)

def get_impromptu_star_info(pid, location_name):
    # ä¸‹è¼‰ 3hr è³‡æ–™ (æŠ—å°é–)
    data = fetch_file_api_data("F-B0053-071") 
    if not data: return "âš ï¸ æ°£è±¡ç½²é€£ç·šå¿™ç¢Œä¸­ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
    
    loc_data = find_location_data(data, pid)
    if not loc_data: return f"âŒ ç„¡è³‡æ–™: {location_name}"

    try:
        raw_elements = loc_data.get('WeatherElement', [])
        elements = {item.get('ElementName'): item.get('Time', []) for item in raw_elements}

        night_status = [] # (æ™‚é–“, å¤©æ°£)
        now = datetime.now(timezone.utc).astimezone(timezone(timedelta(hours=8)))
        
        weather_list = elements.get('å¤©æ°£ç¾è±¡', [])
        for i, item in enumerate(weather_list):
            t_str = item.get('DataTime') or item.get('StartTime')
            if not t_str: continue
            dt = datetime.fromisoformat(t_str)

            # ç¯©é¸æœªä¾†24hå…§çš„æ™šä¸Š
            # ç›´æ¥ç”¨ 86400ï¼Œç”¨å¤©æ•¸åˆ¤æ–·å°±è¦å†å¤šå¯« timedelta çš„ç¨‹å¼ç¢¼
            if dt > now and (dt.hour >= 18 or dt.hour <= 5):
                if (dt - now).total_seconds() > 86400: continue
                
                wx = item['ElementValue']['Weather']
                night_status.append((dt.strftime('%H:%M'), wx))

        # è©•åˆ†èˆ‡å»ºè­°
        perfect_times = [t for t, w in night_status if "æ™´" in w]
        cloudy_times = [t for t, w in night_status if "å¤šé›²" in w and "æ™´" not in w]

        if perfect_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜Š \nå¤ªæ£’äº†ï¼ä»Šæ™šæœ€é©åˆè§€æ˜Ÿçš„æ™‚æ®µç‚ºï¼š{format_time_ranges(perfect_times)}"
        elif cloudy_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜ \nä»Šæ™šé›²é‡è¼ƒå¤šï¼Œè‹¥è¦ç¢°é‹æ°£å¯é¸é€™äº›æ™‚æ®µï¼š{format_time_ranges(cloudy_times)}"
        elif not night_status:
            return f"ğŸ”­ ã€{location_name}ã€‘\nç›®å‰ä¸­å¤®æ°£è±¡ç½²è³‡æ–™æ›´æ–°ä¸­ï¼Œè«‹ç¨æ™šå†è©¦ã€‚"
        else:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜­ \nä»Šæ™šå¤©æ°£ä¸ä½³ï¼ˆé™°å¤©æˆ–é›¨ï¼‰ï¼Œä¸å»ºè­°å‰å¾€è§€æ˜Ÿã€‚"

    except Exception as e: return f"âŒ è§£æéŒ¯èª¤: {e}"

# ==========================================
# ğŸ“… åŠŸèƒ½ Bï¼šæœªä¾†ä¸€é€± (è®€å– CSV + ä¿®æ­£ç‰ˆè¼¸å‡º)
# ==========================================
def get_weekly_star_info(location_name):
    file_name = "all_taiwan_star_forecast.csv"
    
    # æª”æ¡ˆä¸å­˜åœ¨æ™‚è‡ªå‹•æŠ“å– (ä¿®å¾© Render é‡å•Ÿå¾Œè³‡æ–™æ¶ˆå¤±å•é¡Œ)
    if not os.path.exists(file_name):
        update_weekly_csv()
    
    try:
        if not os.path.exists(file_name): return "âš ï¸ è³‡æ–™åº«æš«æ™‚ç„¡æ³•è®€å–ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"
        
        df = pd.read_csv(file_name, encoding="utf-8-sig")
        target_df = df[(df['location'].str.contains(location_name, na=False)) & (df['æ™‚é–“'] == "æ™šä¸Š")].copy()
        
        if target_df.empty: return f"æ‰¾ä¸åˆ°ã€Œ{location_name}ã€çš„è³‡æ–™ã€‚"

        today_str = datetime.now().strftime("%m/%d")
        
        data_list = target_df.head(7).to_dict('records')
        blocks = []
        for item in data_list:
            wx = str(item.get('å¤©æ°£ç‹€æ³', ''))
            
            # --- ä½ çš„è©•åˆ†é‚è¼¯ ---
            score = 1
            eval_msg = ""
            
            if "æ™´" in wx:
                score = 3
                try:
                    fl = float(str(item.get('é«”æ„Ÿæœ€ä½æº«', '20')).replace("..", ""))
                    if fl > 15: score += 1
                    if 20 <= fl <= 25: score += 1
                    
                    if fl < 15: eval_msg = "å¤©æ°£å¯’å†·ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°å¤šç©¿ä¿æš–è¡£ç‰©ï¼"
                    elif 15 <= fl < 20: eval_msg = "å¤©æ°£ç¨æ¶¼ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°ç©¿ä»¶è–„å¤–å¥—ï¼"
                    elif 20 <= fl <= 25: eval_msg = "å¤©æ°£èˆ’é©ï¼Œçµ•ä½³è§€æ˜Ÿæ—¥ï¼"
                    else: eval_msg = "é©åˆè§€æ˜Ÿçš„æº«ç†±å¤œæ™šï¼"
                except: eval_msg = "è«‹æ³¨æ„æ°£æº«è®ŠåŒ–ã€‚"
                
                try:
                    ws = item.get('è’²ç¦é¢¨ç´š', '0')
                    wm = re.findall(r'\d+', str(ws))
                    if wm and int(wm[-1]) >= 5: score -= 1
                except: pass

            elif "å¤šé›²" in wx:
                score = 2
                eval_msg = "ä»Šæ™šé›²é‡è¼ƒå¤šï¼Œå¯ç¢°é‹æ°£ã€‚"
            else:
                score = 1
                eval_msg = "ä»Šæ™šä¸é©åˆè§€æ˜Ÿã€‚"

            score = max(1, min(5, score))
            stars = "â­" * score
            
            # ä¿®æ­£å¾Œçš„ f-string (è£œä¸Šäº†å¼•è™Ÿ)
            res = [
                f"ğŸ“… {item['date']} (æ™šä¸Š)",
                f"å¤©æ°£: {wx}",
                f"æ°£æº«: {item['æœ€ä½æº«']}~{item['æœ€é«˜æº«']}Â°C",
                f"é«”æ„Ÿ: {item.get('é«”æ„Ÿæœ€ä½æº«', '?')}~{item.get('é«”æ„Ÿæœ€é«˜æº«', '?')}Â°C",
                f"é™é›¨: {item['é™é›¨æ©Ÿç‡']}",
                f"è§€æ˜Ÿæ¨è–¦æŒ‡æ•¸: {stars}",
                f"ğŸ“ç¶œåˆè©•ä¼°: {eval_msg}"
            ]
            blocks.append("\n".join(res))
            
        header = f"ğŸŒŒ ã€{location_name}ã€‘æœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å—\n"
        tail = "\n\n----------------\nğŸ”” æº«é¦¨æé†’ï¼šç•¶æ—¥å¯å†ç¢ºèªæ™´æœ—çš„æ™šé–“æ™‚æ®µå“¦ï¼"
        return header + "----------------------\n" + "\n\n".join(blocks) + tail

    except Exception as e:
        return f"âŒ è®€å–è³‡æ–™å¤±æ•—ï¼Œæ­£åœ¨é‡æ–°æŠ“å–...({e})"

if __name__ == "__main__":
    if not CWA_API_KEY:
        print("âŒ è«‹å…ˆè¨­å®š CWA_API_KEY ç’°å¢ƒè®Šæ•¸ï¼")
    else:
        # æ¸¬è©¦ä¸€ä¸‹
        print("æ¸¬è©¦æ›´æ–° CSV...")
        update_weekly_csv()
        print("æ¸¬è©¦è®€å–...")
        print(get_weekly_star_info("é¹¿æ—å¤©æ–‡å°"))