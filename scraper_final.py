import requests
import pandas as pd
import time
import sys
import re
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

# --- é—œéµä¿®æ­£ï¼šåŠ ä¸Š Referer å½è£æˆæ˜¯å¾æ°£è±¡å±€ç¶²ç«™ç™¼å‡ºçš„è«‹æ±‚ ---
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.cwa.gov.tw/V8/C/L/StarView/StarView.html",
    "Accept-Language": "zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7"
}

# --- å€åŸŸåˆ†é¡å­—å…¸ ---
region_map = {
    "åŒ—éƒ¨": ["F010", "F022", "F023", "F011", "F012", "F013", "F001"],
    "ä¸­éƒ¨": ["F014", "F019", "F018", "F020", "F021", "F002", "F016", "F004", "F003"],
    "å—éƒ¨": ["F015", "F017", "F024", "F025", "F026", "F007", "F009", "F008", "F005", "F006"]
}

# --- å…¨å°è§€æ˜Ÿåœ°é»æ¸…å–® ---
all_locations = {
    "F010": "åŸºéš†å¤§æ­¦å´™ç ²å°åœè»Šå ´", "F022": "é™½æ˜å±±åœ‹å®¶å…¬åœ’å°æ²¹å‘åœè»Šå ´", "F023": "é™½æ˜å±±åœ‹å®¶å…¬åœ’æ“å¤©å´—",
    "F011": "äº”åˆ†å±±", "F012": "çŸ³ç¢‡é›²æµ·åœ‹å°", "F013": "çƒä¾†é¢¨æ™¯ç‰¹å®šå€", "F014": "è§€éœ§æ£®æ—éŠæ¨‚å€",
    "F019": "å¤§é›ªå±±åœ‹å®¶æ£®æ—éŠæ¨‚å€", "F018": "æ­¦é™µè¾²å ´", "F020": "ç¦å£½å±±è¾²å ´", "F021": "è‡ºä¸­éƒ½æœƒå…¬åœ’",
    "F002": "å°é¢¨å£åœè»Šå ´", "F016": "æ–°ä¸­æ©«å¡”å¡”åŠ åœè»Šå ´", "F004": "è‡ºå¤§å±±åœ°å¯¦é©—è¾²å ´", "F003": "é³¶å³°åœè»Šå ´",
    "F015": "é˜¿é‡Œå±±éŠæ¨‚å€", "F017": "é¹¿æ—å¤©æ–‡å°", "F024": "ä¸ƒè‚¡æµ·å ¤", "F025": "å—ç€›å¤©æ–‡é¤¨",
    "F026": "è‡ºå—éƒ½æœƒå…¬åœ’", "F007": "é«˜é›„æ¢…å±±é’å¹´æ´»å‹•ä¸­å¿ƒ", "F009": "é«˜é›„éƒ½æœƒå…¬åœ’", "F008": "è—¤ææ£®æ—éŠæ¨‚å€",
    "F005": "å¢¾ä¸è²“é¼»é ­", "F006": "å¢¾ä¸é¾ç£å…¬åœ’", "F001": "å¤ªå¹³å±±æ£®æ—éŠæ¨‚å€",
}

# ==========================================
# åŠŸèƒ½ Aï¼šæ¯é€±é å ± (CSV è®€å–)
# ==========================================

def scrape_weekly_table(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/Week/{pid}_Week_PC.html"
    try:
        response = requests.get(url, headers=headers, timeout=10)
        if response.status_code != 200: return []
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        
        thead = soup.find("thead")
        if not thead: return []
        
        dates = []
        date_rows = thead.find_all("tr")
        if not date_rows: return []
        
        date_row = date_rows[0].find_all("th")
        for th in date_row:
            text = th.get_text(strip=True)
            if not text or text == "æ—¥æœŸ": continue
            for _ in range(int(th.get('colspan', 1))): dates.append(text)
            
        tbody = soup.find("tbody")
        if not tbody: return []
        parsed_data = {}
        
        for row in tbody.find_all("tr"):
            th = row.find("th")
            if not th: continue
            row_name = th.get_text(strip=True)
            vals = []
            for td in row.find_all("td"):
                img = td.find("img")
                tem_c = td.find("span", class_="tem-C") 
                val = img.get('title') or img.get('alt') if img else (tem_c.get_text(strip=True) if tem_c else td.get_text(strip=True))
                if val in ["-", "", None]: val = "æœªçŸ¥"
                if "ç´«å¤–ç·š" in row_name and val != "æœªçŸ¥":
                    match = re.match(r"^(\d+)(.*)$", val)
                    if match: val = f"{match.group(2)}(æŒ‡æ•¸{match.group(1)})"
                vals.append(val)
            parsed_data[row_name] = vals
            
        results = []
        for i in range(len(dates)):
            if i >= len(dates): break
            item = {"location": location_name, "pid": pid, "date": dates[i], "æ™‚é–“": "ç™½å¤©" if i % 2 == 0 else "æ™šä¸Š"}
            for k, v in parsed_data.items(): 
                if k == "æ™‚é–“": continue
                item[k] = v[i] if i < len(v) else "æœªçŸ¥"
            results.append(item)
        return results
    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤ ({location_name}): {e}")
        return []

def update_weekly_csv():
    file_name = "all_taiwan_star_forecast.csv"
    print(f"ğŸš€ é–‹å§‹æ›´æ–°æ¯é€±é å ±è³‡æ–™ (å…± {len(all_locations)} è™•)...")
    final_data = []
    for pid, name in all_locations.items():
        data = scrape_weekly_table(pid, name)
        if data: final_data.extend(data)
        time.sleep(0.1)
    
    if final_data:
        new_df = pd.DataFrame(final_data)
        final_df = new_df 
        final_df.to_csv(file_name, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV æ›´æ–°å®Œæˆï¼ç›®å‰å…±æœ‰ {len(final_df)} ç­†æ•¸æ“šã€‚")

def get_weekly_star_info(user_input):
    file_name = "all_taiwan_star_forecast.csv"
    try:
        if not os.path.exists(file_name): return "âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™æª”ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡æ›´æ–°è³‡æ–™åº«ã€‚"
        df = pd.read_csv(file_name, encoding="utf-8-sig")
        target_df = df[(df['location'].str.contains(user_input, na=False)) & (df['æ™‚é–“'] == "æ™šä¸Š")].copy()
        
        if target_df.empty: return f"æ‰¾ä¸åˆ°ã€Œ{user_input}ã€çš„é å ±è³‡æ–™ã€‚"

        tz_taiwan = timezone(timedelta(hours=8))
        today = datetime.now(tz_taiwan).date()
        this_year = datetime.now(tz_taiwan).year

        def is_future(date_str):
            try: return datetime.strptime(f"{this_year}/{date_str[:5]}", "%Y/%m/%d").date() >= today
            except: return True
            
        target_df = target_df[target_df['date'].apply(is_future)]
        data_list = target_df.head(7).to_dict('records')
        
        if not data_list: return f"ç›®å‰æ²’æœ‰ {user_input} æœªä¾†ä¸€é€±çš„è³‡æ–™ã€‚"

        all_blocks = []
        for item in data_list:
            weather = str(item.get('å¤©æ°£ç‹€æ³', 'æœªçŸ¥'))
            score = 1
            eval_msg = ""
            
            if "æ™´" in weather:
                score = 3
                try:
                    t_str = str(item.get('æœ€é«˜æº«', 0)).replace("..", "")
                    t_high = int(float(t_str)) if t_str.isdigit() else 20
                    if t_high > 15: score += 1
                    if 20 <= t_high <= 25: score += 1
                except: pass
                try:
                    wind_str = str(item.get('è’²ç¦é¢¨ç´š', '0'))
                    wind_matches = re.findall(r'\d+', wind_str)
                    if wind_matches and int(wind_matches[-1]) >= 5: score -= 1
                except: pass
                try:
                    fl_str = str(item.get('é«”æ„Ÿæœ€ä½æº«', 20)).replace("..", "")
                    fl = int(float(fl_str)) if fl_str.replace('.', '', 1).isdigit() else 20
                    if fl < 15: eval_msg = "å¤©æ°£å¯’å†·ï¼Œå»ºè­°å¤šç©¿ä¿æš–è¡£ç‰©ï¼"
                    elif 15 <= fl < 20: eval_msg = "å¤©æ°£ç¨æ¶¼ï¼Œå»ºè­°ç©¿ä»¶è–„å¤–å¥—ï¼"
                    elif 20 <= fl <= 25: eval_msg = "å¤©æ°£èˆ’é©ï¼Œçµ•ä½³è§€æ˜Ÿæ—¥ï¼"
                    else: eval_msg = "é©åˆè§€æ˜Ÿçš„æº«ç†±å¤œæ™šï¼"
                except: eval_msg = "è«‹æ³¨æ„ç¾å ´å¤©æ°£è®ŠåŒ–ã€‚"

            elif "å¤šé›²" in weather:
                score = 2
                eval_msg = "é›²é‡è¼ƒå¤šï¼Œå¯ç¢°ç¢°é‹æ°£ã€‚"
            else:
                score = 1
                eval_msg = "ä»Šæ™šä¸é©åˆè§€æ˜Ÿã€‚"

            stars = "â­" * max(1, min(5, score))
            res = [
                f"ğŸ“… {item['date']} ({item['æ™‚é–“']})",
                f"å¤©æ°£ï¼š{weather}",
                f"æ°£æº«ï¼š{item.get('æœ€ä½æº«', '?')}~{item.get('æœ€é«˜æº«', '?')}Â°C",
                f"é«”æ„Ÿï¼š{item.get('é«”æ„Ÿæœ€ä½æº«', '?')}~{item.get('é«”æ„Ÿæœ€é«˜æº«', '?')}Â°C",
                f"é™é›¨ï¼š{item.get('é™é›¨æ©Ÿç‡', 'æœªçŸ¥')}",
                f"è§€æ˜Ÿæ¨è–¦æŒ‡æ•¸ï¼š{stars}",
                f"ğŸ“ è©•ä¼°ï¼š{eval_msg}"
            ]
            all_blocks.append("\n".join(res))
            
        header = f"ğŸŒŒ ã€{user_input}ã€‘æœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å—\n\n"
        tail = "\n\n----------------\nğŸ”” æº«é¦¨æé†’ï¼šå±±å€å¤©æ°£å¤šè®Šï¼Œå‡ºç™¼å‰è«‹å†æ¬¡ç¢ºèªï¼\n\n"
        return header + "\n\n----------------\n".join(all_blocks) + tail
    except Exception as e: return f"âŒ éŒ¯èª¤ï¼š{str(e)}"


# ==========================================
# åŠŸèƒ½ Bï¼šè‡¨æ™‚èˆˆèµ· (72hr å³æ™‚çˆ¬èŸ² - ä¿®å¾©ç‰ˆ)
# ==========================================

def format_time_ranges(time_list):
    if not time_list: return ""
    hours = []
    for t in time_list:
        try:
            h_str = t.split(':')[0]
            hours.append(int(h_str))
        except: continue
        
    if not hours: return ""

    has_evening = any(h >= 18 for h in hours)
    processed = [h + 24 if (h <= 5 and has_evening) else h for h in hours]
    processed.sort()
    
    ranges = []
    if not processed: return ""

    start_h = prev_h = processed[0]
    for i in range(1, len(processed)):
        curr = processed[i]
        if curr == prev_h + 1:
            prev_h = curr
        else:
            ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
            start_h = prev_h = curr
    ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
    return "ã€".join(ranges)

def get_impromptu_star_info(pid, location_name):
    # åŠ ä¸Šäº‚æ•¸åƒæ•¸é¿å…å¿«å–
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/3hr/{pid}_3hr_PC.html?t={int(time.time())}"
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        
        if resp.status_code != 200:
            return f"âŒ ç„¡æ³•å–å¾—è³‡æ–™ (ç‹€æ…‹ç¢¼: {resp.status_code})ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

        raw_html = f"<table>{resp.text}</table>" if "<table" not in resp.text else resp.text
        soup = BeautifulSoup(raw_html, "html.parser")
        
        # --- 1. æŠ“å–æ—¥æœŸå°ç…§è¡¨ ---
        date_map = {}
        for th in soup.find_all("th"):
            th_id = th.get("id")
            if th_id and "PC3_D" in th_id and "H" not in th_id and th_id != "PC3_D":
                key = th_id.split("_")[-1] 
                date_map[key] = th.get_text(strip=True)[:5]

        # --- 2. æŠ“å–æ™‚é–“åˆ— (æ ¸å¿ƒé˜²å‘†ä¿®æ­£) ---
        time_row = soup.find("tr", class_="time")
        
        # ğŸ”¥ å¦‚æœæ‰¾ä¸åˆ°æ™‚é–“åˆ—ï¼Œä»£è¡¨ç¶²ç«™çµæ§‹æ”¹è®Šæˆ–è¢«æ“‹ï¼Œç›´æ¥å›å‚³æç¤ºï¼Œä¸è¦ç•¶æ©Ÿ
        if not time_row:
            return f"âš ï¸ æš«æ™‚ç„¡æ³•è®€å– {location_name} çš„å³æ™‚è³‡æ–™ï¼ˆä¾†æºç¶²ç«™ç„¡å›æ‡‰ï¼‰ï¼Œè«‹æ”¹ç”¨ã€Œæœªä¾†ä¸€é€±ã€åŠŸèƒ½ã€‚"

        time_full_labels = {}
        time_ids = []
        
        for th in time_row.find_all("th"):
            tid = th.get('id')
            if not tid: continue
            matched_date_key = next((dk for dk in date_map if dk in tid), None)
            if matched_date_key:
                time_str = th.get_text(strip=True)
                time_full_labels[tid] = f"{date_map[matched_date_key]} {time_str}"
                time_ids.append(tid)

        # --- 3. æŠ“å–å¤©æ°£æ•¸æ“š ---
        master_data = {tid: {} for tid in time_ids}
        for row in soup.find_all("tr"):
            th = row.find("th")
            if not th: continue
            title = th.get_text(strip=True)
            if "æ™‚é–“" in title: continue

            for td in row.find_all("td"):
                h_attr = td.get('headers', "")
                val = "æœªçŸ¥"
                img = td.find("img")
                span = td.find("span", class_="tem-C")
                if img: val = img.get('title')
                elif span: val = span.get_text(strip=True)
                else: val = td.get_text(strip=True)

                for tid in time_ids:
                    if tid in h_attr:
                        master_data[tid][title] = val

        # --- 4. ç¯©é¸ä»Šæ™š ---
        night_status = []
        check_ids = time_ids[:24] if len(time_ids) > 24 else time_ids
        for tid in check_ids:
            if tid not in time_full_labels: continue
            full_time_str = time_full_labels[tid] 
            try:
                t_part = full_time_str.split(" ")[1]
                hour = int(t_part.split(":")[0])
                if hour >= 18 or hour <= 5:
                    w = master_data[tid].get("å¤©æ°£ç‹€æ³", "æœªçŸ¥")
                    night_status.append((t_part, w))
            except: continue

        # --- 5. ç”¢ç”Ÿå»ºè­° ---
        perfect_times = [t for t, w in night_status if "æ™´" in w]
        cloudy_times = [t for t, w in night_status if "å¤šé›²" in w and "æ™´" not in w]
        
        if perfect_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜Š \nå¤ªæ£’äº†ï¼ä»Šæ™šæœ€é©åˆè§€æ˜Ÿçš„æ™‚æ®µç‚ºï¼š{format_time_ranges(perfect_times)}"
        elif cloudy_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜ \nä»Šæ™šé›²é‡è¼ƒå¤šï¼Œè‹¥è¦ç¢°é‹æ°£å¯é¸é€™äº›æ™‚æ®µï¼š{format_time_ranges(cloudy_times)}"
        elif not night_status:
            return f"ğŸ”­ ã€{location_name}ã€‘\nç›®å‰æ°£è±¡å±€å°šæœªæ›´æ–°ä»Šæ™šçš„è©³ç´°è³‡æ–™ï¼Œè«‹ç¨æ™šå†è©¦ã€‚"
        else:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜­ \nä»Šæ™šå¤©æ°£ä¸ä½³ï¼ˆé™°å¤©æˆ–é›¨ï¼‰ï¼Œä¸å»ºè­°å‰å¾€è§€æ˜Ÿã€‚"

    except Exception as e:
        # å›å‚³éŒ¯èª¤è¨Šæ¯çµ¦ä½¿ç”¨è€…ï¼Œè€Œä¸æ˜¯è®“ç¨‹å¼å´©æ½°
        return f"âŒ æŠ±æ­‰ï¼ŒæŸ¥è©¢ {location_name} æ™‚ç™¼ç”Ÿè³‡æ–™è®€å–éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚"

# ==========================================
# ä¸»ç¨‹å¼æ¸¬è©¦å€
# ==========================================

if __name__ == "__main__":
    # 1. ç¬¬ä¸€æ¬¡åŸ·è¡Œå»ºè­°è·‘ä¸€æ¬¡æ›´æ–°
    # update_weekly_csv() 
    
    print("\n--------- æ¨¡æ“¬ LINE Bot ä½¿ç”¨è€…æ“ä½œ ---------")
    
    # æ¸¬è©¦ Aï¼šæœªä¾†ä¸€é€± (æ¸¬è©¦æ˜Ÿæ˜Ÿé‚è¼¯)
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šæœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å— -> é¸æ“‡ï¼šé™½æ˜å±±å°æ²¹å‘")
    print(get_weekly_star_info("å°æ²¹å‘"))
    
    print("\n-------------------------------------------")
    
    # æ¸¬è©¦ Bï¼šè‡¨æ™‚å‡ºç™¼ (æ¸¬è©¦æ™‚æ®µåˆä½µèˆ‡æ–‡å­—é‚è¼¯)
    # å»ºè­°æ‰¾ä¸€å€‹ç¾åœ¨æ˜¯æ™šä¸Šçš„æ™‚é–“æ¸¬è©¦ï¼Œæˆ–è€…æ‰¾é¹¿æ—å¤©æ–‡å°é€™ç¨®å®¹æ˜“æœ‰æ™´å¤©çš„
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šè‡¨æ™‚èˆˆèµ·å»è§€æ˜Ÿ -> é¸æ“‡ï¼šé¹¿æ—å¤©æ–‡å°")
    print(get_impromptu_star_info("F017", "é¹¿æ—å¤©æ–‡å°"))