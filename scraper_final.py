import requests
import pandas as pd
import time
import sys
import re
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

# --- å€åŸŸåˆ†é¡å­—å…¸ (ç¶­æŒåŸæ¨£) ---
region_map = {
    "åŒ—éƒ¨": ["F010", "F022", "F023", "F011", "F012", "F013", "F001"],
    "ä¸­éƒ¨": ["F014", "F019", "F018", "F020", "F021", "F002", "F016", "F004", "F003"],
    "å—éƒ¨": ["F015", "F017", "F024", "F025", "F026", "F007", "F009", "F008", "F005", "F006"]
}

# --- å…¨å°è§€æ˜Ÿåœ°é»æ¸…å–® (ç¶­æŒåŸæ¨£) ---
all_locations = {
    "F010": "åŸºéš†å¤§æ­¦å´™ç ²å°åœè»Šå ´", "F022": "é™½æ˜å±±åœ‹å®¶å…¬åœ’å°æ²¹å‘åœè»Šå ´", "F023": "é™½æ˜å±±åœ‹å®¶å…¬åœ’æ“å¤©å´—",
    "F011": "äº”åˆ†å±±", "F012": "çŸ³ç¢‡é›²æµ·åœ‹å°", "F013": "çƒä¾†é¢¨æ™¯ç‰¹å®šå€", "F014": "è§€éœ§æ£®æ—éŠæ¨‚å€",
    "F019": "å¤§é›ªå±±åœ‹å®¶æ£®æ—éŠæ¨‚å€", "F018": "æ­¦é™µè¾²å ´", "F020": "ç¦å£½å±±è¾²å ´", "F021": "è‡ºä¸­éƒ½æœƒå…¬åœ’",
    "F002": "å°é¢¨å£åœè»Šå ´", "F016": "æ–°ä¸­æ©«å¡”å¡”åŠ åœè»Šå ´", "F004": "è‡ºå¤§å±±åœ°å¯¦é©—è¾²å ´", "F003": "é³¶å³°åœè»Šå ´",
    "F015": "é˜¿é‡Œå±±éŠæ¨‚å€", "F017": "é¹¿æ—å¤©æ–‡å°", "F024": "ä¸ƒè‚¡æµ·å ¤", "F025": "å—ç€›å¤©æ–‡é¤¨",
    "F026": "è‡ºå—éƒ½æœƒå…¬åœ’", "F007": "é«˜é›„æ¢…å±±é’å¹´æ´»å‹•ä¸­å¿ƒ", "F009": "é«˜é›„éƒ½æœƒå…¬åœ’", "F008": "è—¤ææ£®æ—éŠæ¨‚å€",
    "F005": "å¢¾ä¸è²“é¼»é ­", "F006": "å¢¾ä¸é¾ç£å…¬åœ’", "F001": "å¤ªå¹³å±±æ£®æ—éŠæ¨‚å€",
}

# ==========================================
# åŠŸèƒ½ Aï¼šæ¯é€±é å ± (CSV è®€å–)
# ==========================================

def scrape_weekly_table(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/Week/{pid}_Week_PC.html"
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200: return []
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        
        thead = soup.find("thead")
        if not thead: return []
        dates = []
        date_row = thead.find_all("tr")[0].find_all("th")
        for th in date_row:
            text = th.get_text(strip=True)
            if not text or text == "æ—¥æœŸ": continue
            for _ in range(int(th.get('colspan', 1))): dates.append(text)
            
        tbody = soup.find("tbody")
        if not tbody: return []
        parsed_data = {}
        
        for row in tbody.find_all("tr"):
            th = row.find("th")
            if not th: continue
            row_name = th.get_text(strip=True)
            vals = []
            for td in row.find_all("td"):
                img = td.find("img")
                tem_c = td.find("span", class_="tem-C") 
                val = img.get('title') or img.get('alt') if img else (tem_c.get_text(strip=True) if tem_c else td.get_text(strip=True))
                if val in ["-", "", None]: val = "æœªçŸ¥"
                if "ç´«å¤–ç·š" in row_name and val != "æœªçŸ¥":
                    match = re.match(r"^(\d+)(.*)$", val)
                    if match: val = f"{match.group(2)}(æŒ‡æ•¸{match.group(1)})"
                vals.append(val)
            parsed_data[row_name] = vals
            
        results = []
        for i in range(len(dates)):
            item = {"location": location_name, "pid": pid, "date": dates[i], "æ™‚é–“": "ç™½å¤©" if i % 2 == 0 else "æ™šä¸Š"}
            for k, v in parsed_data.items(): 
                if k == "æ™‚é–“": continue
                item[k] = v[i] if i < len(v) else "æœªçŸ¥"
            results.append(item)
        return results
    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤ ({location_name}): {e}")
        return []

def update_weekly_csv():
    file_name = "all_taiwan_star_forecast.csv"
    print(f"ğŸš€ é–‹å§‹æ›´æ–°æ¯é€±é å ±è³‡æ–™ (å…± {len(all_locations)} è™•)...")
    final_data = []
    for pid, name in all_locations.items():
        data = scrape_weekly_table(pid, name)
        if data: final_data.extend(data)
        time.sleep(0.2)
    
    if final_data:
        new_df = pd.DataFrame(final_data)
        if os.path.exists(file_name) and os.path.getsize(file_name) > 0:
            try:
                final_df = pd.concat([pd.read_csv(file_name, encoding="utf-8-sig"), new_df], ignore_index=True).drop_duplicates(subset=['location', 'date', 'æ™‚é–“'], keep='last')
            except: final_df = new_df
        else: final_df = new_df
        final_df.to_csv(file_name, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV æ›´æ–°å®Œæˆï¼ç›®å‰å…±æœ‰ {len(final_df)} ç­†æ•¸æ“šã€‚")

def get_weekly_star_info(user_input):
    file_name = "all_taiwan_star_forecast.csv"
    try:
        if not os.path.exists(file_name): return "âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™æª”ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡æ›´æ–°è³‡æ–™åº«ã€‚"
        df = pd.read_csv(file_name, encoding="utf-8-sig")
        # åƒ…ç¯©é¸æ™šä¸Š
        target_df = df[(df['location'].str.contains(user_input)) & (df['æ™‚é–“'] == "æ™šä¸Š")].copy()
        
        if target_df.empty: return f"æ‰¾ä¸åˆ°ã€Œ{user_input}ã€çš„é å ±è³‡æ–™ã€‚"

        tz_taiwan = timezone(timedelta(hours=8))
        today = datetime.now(tz_taiwan).date()
        this_year = datetime.now(tz_taiwan).year

        def is_future(date_str):
            try: return datetime.strptime(f"{this_year}/{date_str[:5]}", "%Y/%m/%d").date() >= today
            except: return True
            
        target_df = target_df[target_df['date'].apply(is_future)]
        data_list = target_df.head(7).to_dict('records')
        
        if not data_list: return f"ç›®å‰æ²’æœ‰ {user_input} æœªä¾†ä¸€é€±çš„è³‡æ–™ã€‚"

        all_blocks = []
        for item in data_list:
            weather = item.get('å¤©æ°£ç‹€æ³', 'æœªçŸ¥')
            
            # --- ğŸ’¡ ä¿®æ­£å¾Œçš„æ ¸å¿ƒåˆ¤å®šé‚è¼¯ ---
            # å„ªå…ˆé †åºï¼šæ™´ (ç´°ç®—) > å¤šé›² (2æ˜Ÿ) > å…¶ä»– (1æ˜Ÿ)
            
            score = 1 # é è¨­ (é™°/é›¨)
            eval_msg = "" # è©•åƒ¹æ–‡å­—
            
            if "æ™´" in weather:
                score = 3 # åŸºç¤åˆ† (åŸæœ¬æ˜¯1+2)
                # åªæœ‰æ™´å¤©ç¹¼çºŒåˆ¤æ–·æ°£æº«èˆ‡é¢¨åŠ›
                try:
                    # æ°£æº«åŠ åˆ†
                    t_high = int(item.get('æœ€é«˜æº«', 0))
                    if t_high > 15: score += 1
                    if 20 <= t_high <= 25: score += 1
                except: pass
                
                try:
                    # é¢¨åŠ›æ‰£åˆ†
                    if int(re.findall(r'\d+', str(item.get('è’²ç¦é¢¨ç´š', '0')))[-1]) >= 5: score -= 1
                except: pass

                # æ™´å¤©æ™‚çš„ç¶œåˆè©•ä¼°æ–‡å­—
                try:
                    fl = int(item.get('é«”æ„Ÿæœ€ä½æº«', 20))
                    if fl < 15: eval_msg = "å¤©æ°£å¯’å†·ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°å¤šç©¿å¹¾ä»¶ä¿æš–è¡£ç‰©ï¼"
                    elif 15 <= fl < 20: eval_msg = "å¤©æ°£ç¨æ¶¼ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°ç©¿ä»¶è–„å¤–å¥—ï¼"
                    elif 20 <= fl <= 25: eval_msg = "å¤©æ°£èˆ’é©ï¼Œçµ•ä½³è§€æ˜Ÿæ—¥ï¼"
                    else: eval_msg = "é©åˆè§€æ˜Ÿçš„æº«ç†±å¤œæ™šï¼"
                except: eval_msg = "è«‹æ³¨æ„ç¾å ´å¤©æ°£è®ŠåŒ–ã€‚"

                # é¢¨åŠ›è­¦ç¤º (åƒ…åœ¨æ™´å¤©ä¸”é¢¨å¤§æ™‚æé†’)
                try:
                    if int(re.findall(r'\d+', str(item.get('è’²ç¦é¢¨ç´š', '0')))[-1]) >= 5:
                        eval_msg += " (å¦å¤–ä»Šæ™šé¢¨åŠ›è¼ƒå¼·ï¼Œè¡Œç¶“è¦–ç·šæ˜æš—è™•è«‹å°å¿ƒï¼)"
                except: pass

            elif "å¤šé›²" in weather:
                score = 2 # å¤šé›²å›ºå®š 2 é¡†æ˜Ÿ
                eval_msg = "é›²é‡è¼ƒå¤šï¼Œå¯èƒ½å½±éŸ¿è§€æ˜Ÿé«”é©—ï¼Œå¯ç¢°ç¢°é‹æ°£ã€‚"
            
            else:
                score = 1 # é™°å¤©æˆ–é›¨å¤©å›ºå®š 1 é¡†æ˜Ÿ
                eval_msg = "ä»Šæ™šä¸é©åˆè§€æ˜Ÿã€‚"

            # æ˜Ÿæ˜Ÿä¸Šé™ 5 é¡†
            stars = "â­" * max(1, min(5, score))

            # --- æ ¼å¼çµ„è£ ---
            res = [
                f"ğŸ“… {item['date']} ({item['æ™‚é–“']})",
                f"å¤©æ°£ï¼š{weather}",
                f"æ°£æº«ï¼š{item.get('æœ€ä½æº«', '?')}~{item.get('æœ€é«˜æº«', '?')}Â°C",
                f"é«”æ„Ÿï¼š{item.get('é«”æ„Ÿæœ€ä½æº«', '?')}~{item.get('é«”æ„Ÿæœ€é«˜æº«', '?')}Â°C",
                f"é™é›¨ï¼š{item.get('é™é›¨æ©Ÿç‡', 'æœªçŸ¥')}",
                f"è§€æ˜Ÿæ¨è–¦æŒ‡æ•¸ï¼š{stars}",
                f"ğŸ“ ç¶œåˆè©•ä¼°ï¼š{eval_msg}"
            ]
            all_blocks.append("\n".join(res))
            
        header = f"ğŸŒŒ ã€{user_input}ã€‘æœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å—\n\n"
        tail = "\n\n----------------\nğŸ”” æº«é¦¨æé†’ï¼šç•¶æ—¥å¯å†ç¢ºèªæ™´æœ—çš„æ™šé–“æ™‚æ®µå“¦~\n\n"
        return header + "\n\n----------------\n".join(all_blocks) + tail
    except Exception as e: return f"âŒ éŒ¯èª¤ï¼š{str(e)}"


# ==========================================
# åŠŸèƒ½ Bï¼šè‡¨æ™‚èˆˆèµ· (72hr å³æ™‚çˆ¬èŸ²)
# ==========================================

def format_time_ranges(time_list):
    if not time_list: return ""
    hours = [int(t.split(':')[0]) for t in time_list]
    
    # è™•ç†è·¨å¤œï¼šå°‡ 00:00~05:00 è½‰æ›ç‚º 24, 25... ä»¥ä¾¿æ’åº
    # é‚è¼¯ï¼šå¦‚æœæ¸…å–®ä¸­åŒæ™‚å­˜åœ¨ã€Œæ™šä¸Š(>=18)ã€å’Œã€Œå‡Œæ™¨(<=5)ã€ï¼Œæ‰æŠŠå‡Œæ™¨åŠ  24
    has_evening = any(h >= 18 for h in hours)
    processed = [h + 24 if (h <= 5 and has_evening) else h for h in hours]
    
    # ğŸ’¡ é—œéµä¿®æ­£ï¼šå¿…é ˆæ’åºï¼å¦å‰‡ 25, 26 è‹¥åœ¨ 18, 19 å‰é¢ï¼Œæœƒæ–·æˆå…©æˆª
    processed.sort() 
    
    ranges = []
    if not processed: return ""

    start_h = prev_h = processed[0]
    for i in range(1, len(processed)):
        curr = processed[i]
        if curr == prev_h + 1:
            prev_h = curr
        else:
            # çµæŸä¸€æ®µé€£çºŒæ™‚é–“
            ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
            start_h = prev_h = curr
    
    # åŠ å…¥æœ€å¾Œä¸€æ®µ
    ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
    
    return "ã€".join(ranges)

def get_impromptu_star_info(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/3hr/{pid}_3hr_PC.html"
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        raw_html = f"<table>{resp.text}</table>" if "<table" not in resp.text else resp.text
        soup = BeautifulSoup(raw_html, "html.parser")
        
        date_map = {th.get("id").split("_")[-1]: th.get_text(strip=True)[:5] 
                    for th in soup.find_all("th") if th.get("id") and "PC3_D" in th.get("id") and "H" not in th.get("id") and th.get("id") != "PC3_D"}
        time_row = soup.find("tr", class_="time")
        time_ids = [th.get('id') for th in time_row.find_all("th")[1:] if th.get('id')]
        time_full_labels = {tid: f"{date_map[next(dk for dk in date_map if dk in tid)]} {time_row.find('th', id=tid).get_text(strip=True)}" for tid in time_ids}

        master_data = {tid: {} for tid in time_ids}
        for row in soup.find_all("tr"):
            th = row.find("th")
            if not th or "æ™‚é–“" in th.get_text(): continue
            title = th.get_text(strip=True)
            for td in row.find_all("td"):
                h_attr = td.get('headers', "")
                val = td.find("img").get('title') if td.find("img") else (td.find("span", class_="tem-C").get_text(strip=True) if td.find("span", class_="tem-C") else td.get_text(strip=True))
                for tid in time_ids:
                    if tid in h_attr: master_data[tid][title] = val

        # æ”¶é›†ä»Šæ™šæ‰€æœ‰æ™‚æ®µ (18:00 - 05:00)
        night_status = [] # æ ¼å¼: (æ™‚é–“å­—ä¸², å¤©æ°£ç‹€æ³)
        
        for tid in time_ids[:24]: # åªçœ‹æœ€è¿‘ 24 å°æ™‚å…§çš„
            time_str = time_full_labels[tid].split(" ")[1]
            hour = int(time_str.split(":")[0])
            if hour >= 18 or hour <= 5:
                w = master_data[tid].get("å¤©æ°£ç‹€æ³", "æœªçŸ¥")
                night_status.append((time_str, w))

        # ç¯©é¸ç‰¹å®šå¤©æ°£çš„æ™‚æ®µ
        perfect_times = [t for t, w in night_status if "æ™´" in w]
        cloudy_times = [t for t, w in night_status if "å¤šé›²" in w and "æ™´" not in w]
        
        # --- ğŸ’¡ ä¿®æ­£å¾Œçš„è©•ä¼°é‚è¼¯ ---
        if perfect_times:
            # åªè¦æœ‰ã€Œæ™´ã€çš„æ™‚æ®µï¼Œå°±å„ªå…ˆé¡¯ç¤º
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜Š \nå¤ªæ£’äº†ï¼Œä»Šæ™šæœ€é©åˆè§€æ˜Ÿçš„æ™‚æ®µç‚ºï¼š{format_time_ranges(perfect_times)}"
        
        elif cloudy_times:
            # æ²’æœ‰æ™´ï¼Œä½†æœ‰ã€Œå¤šé›²ã€
            # é€™è£¡åªé¡¯ç¤ºæç¤ºèªï¼Œä¹Ÿå¯ä»¥é¸æ“‡åˆ—å‡ºå¤šé›²æ™‚æ®µ format_time_ranges(cloudy_times)
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜ \nä»Šæ™šå„æ™‚æ®µé›²é‡å‡è¼ƒå¤šï¼Œå¯å‡ºé–€ç¢°ç¢°é‹æ°£! (æ™‚æ®µ: {format_time_ranges(cloudy_times)})"
        
        else:
            # å‰©ä¸‹çš„éƒ½æ˜¯é™°æˆ–é›¨
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜­ \nä»Šæ™šä¸é©åˆè§€æ˜Ÿï¼Œè«‹å¥½å¥½ç¡è¦ºã€‚"

    except Exception as e:
        return f"âŒ ç³»çµ±éŒ¯èª¤: {str(e)}"

# ==========================================
# ä¸»ç¨‹å¼æ¸¬è©¦å€
# ==========================================
if __name__ == "__main__":
    # 1. ç¬¬ä¸€æ¬¡åŸ·è¡Œå»ºè­°è·‘ä¸€æ¬¡æ›´æ–°
    # update_weekly_csv() 
    
    print("\n--------- æ¨¡æ“¬ LINE Bot ä½¿ç”¨è€…æ“ä½œ ---------")
    
    # æ¸¬è©¦ Aï¼šæœªä¾†ä¸€é€± (æ¸¬è©¦æ˜Ÿæ˜Ÿé‚è¼¯)
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šæœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å— -> é¸æ“‡ï¼šé™½æ˜å±±å°æ²¹å‘")
    print(get_weekly_star_info("å°æ²¹å‘"))
    
    print("\n-------------------------------------------")
    
    # æ¸¬è©¦ Bï¼šè‡¨æ™‚å‡ºç™¼ (æ¸¬è©¦æ™‚æ®µåˆä½µèˆ‡æ–‡å­—é‚è¼¯)
    # å»ºè­°æ‰¾ä¸€å€‹ç¾åœ¨æ˜¯æ™šä¸Šçš„æ™‚é–“æ¸¬è©¦ï¼Œæˆ–è€…æ‰¾é¹¿æ—å¤©æ–‡å°é€™ç¨®å®¹æ˜“æœ‰æ™´å¤©çš„
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šè‡¨æ™‚èˆˆèµ·å»è§€æ˜Ÿ -> é¸æ“‡ï¼šé¹¿æ—å¤©æ–‡å°")
    print(get_impromptu_star_info("F017", "é¹¿æ—å¤©æ–‡å°"))