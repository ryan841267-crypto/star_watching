import requests
import pandas as pd
import time
import sys
import re
import os
from bs4 import BeautifulSoup
from datetime import datetime, timedelta, timezone

# ä¿®æ­£ Windows è¼¸å‡ºç·¨ç¢¼
sys.stdout.reconfigure(encoding='utf-8')

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}


# ç”¨ä¾†è™•ç† Quick Reply çš„å€åŸŸåˆ†é¡
# å€åŸŸåˆ†é¡å­—å…¸ (ç”¨æ–¼ LINE Bot é¸å–®é‚è¼¯)
# é‚è¼¯ï¼šåŒ—ä¸­å—ä¸‰å€ï¼Œå‰›å¥½ç¬¦åˆ LINE Carousel ä¸Šé™ (10å€‹)
region_map = {
    "åŒ—éƒ¨": [
        "F010", # åŸºéš†
        "F022", "F023", # å°åŒ—
        "F011", "F012", "F013", # æ–°åŒ—
        "F001"  # å®œè˜­
    ],
    "ä¸­éƒ¨": [
        "F014", # è‹—æ —
        "F019", "F018", "F020", "F021", # å°ä¸­
        "F002", "F016", "F004", "F003"  # å—æŠ•
    ],
    "å—éƒ¨": [
        "F015", "F017", # å˜‰ç¾©
        "F024", "F025", "F026", # å°å—
        "F007", "F009", "F008", # é«˜é›„
        "F005", "F006"  # å±æ±
    ]
}

# åå‘æŸ¥è©¢ (å¦‚æœéœ€è¦å¾ PID æ‰¾å€åŸŸåç¨±)
# ä½ çš„ all_locations å·²ç¶“æœ‰äº† PID å°æ‡‰åç¨±ï¼Œé€™éƒ¨åˆ†ç¶­æŒåŸæ¨£å³å¯
# --- 1. å…¨å°è§€æ˜Ÿåœ°é»æ¸…å–® (å…±ç”¨) ---
all_locations = {
    "F010": "åŸºéš†å¤§æ­¦å´™ç ²å°åœè»Šå ´", "F022": "é™½æ˜å±±åœ‹å®¶å…¬åœ’å°æ²¹å‘åœè»Šå ´", "F023": "é™½æ˜å±±åœ‹å®¶å…¬åœ’æ“å¤©å´—",
    "F011": "äº”åˆ†å±±", "F012": "çŸ³ç¢‡é›²æµ·åœ‹å°", "F013": "çƒä¾†é¢¨æ™¯ç‰¹å®šå€", "F014": "è§€éœ§æ£®æ—éŠæ¨‚å€",
    "F019": "å¤§é›ªå±±åœ‹å®¶æ£®æ—éŠæ¨‚å€", "F018": "æ­¦é™µè¾²å ´", "F020": "ç¦å£½å±±è¾²å ´", "F021": "è‡ºä¸­éƒ½æœƒå…¬åœ’",
    "F002": "å°é¢¨å£åœè»Šå ´", "F016": "æ–°ä¸­æ©«å¡”å¡”åŠ åœè»Šå ´", "F004": "è‡ºå¤§å±±åœ°å¯¦é©—è¾²å ´", "F003": "é³¶å³°åœè»Šå ´",
    "F015": "é˜¿é‡Œå±±éŠæ¨‚å€", "F017": "é¹¿æ—å¤©æ–‡å°", "F024": "ä¸ƒè‚¡æµ·å ¤", "F025": "å—ç€›å¤©æ–‡é¤¨",
    "F026": "è‡ºå—éƒ½æœƒå…¬åœ’", "F007": "é«˜é›„æ¢…å±±é’å¹´æ´»å‹•ä¸­å¿ƒ", "F009": "é«˜é›„éƒ½æœƒå…¬åœ’", "F008": "è—¤ææ£®æ—éŠæ¨‚å€",
    "F005": "å¢¾ä¸è²“é¼»é ­", "F006": "å¢¾ä¸é¾ç£å…¬åœ’", "F001": "å¤ªå¹³å±±æ£®æ—éŠæ¨‚å€",
}

# ==========================================
# åŠŸèƒ½ Aï¼šæ¯é€±é å ± (CSV è®€å–)
# ==========================================

# çˆ¬èŸ²å‡½å¼ (æ›´æ–° CSV ç”¨)
def scrape_weekly_table(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/Week/{pid}_Week_PC.html"
    try:
        response = requests.get(url, headers=headers)
        if response.status_code != 200: return []
        response.encoding = 'utf-8'
        soup = BeautifulSoup(response.text, "html.parser")
        
        thead = soup.find("thead")
        if not thead: return []
        dates = []
        date_row = thead.find_all("tr")[0].find_all("th")
        for th in date_row:
            text = th.get_text(strip=True)
            if not text or text == "æ—¥æœŸ": continue
            for _ in range(int(th.get('colspan', 1))): dates.append(text)
            
        tbody = soup.find("tbody")
        if not tbody: return []
        parsed_data = {}
        
        for row in tbody.find_all("tr"):
            th = row.find("th")
            if not th: continue
            row_name = th.get_text(strip=True)
            vals = []
            for td in row.find_all("td"):
                img = td.find("img")
                tem_c = td.find("span", class_="tem-C") 
                val = img.get('title') or img.get('alt') if img else (tem_c.get_text(strip=True) if tem_c else td.get_text(strip=True))
                if val in ["-", "", None]: val = "æœªçŸ¥"
                if "ç´«å¤–ç·š" in row_name and val != "æœªçŸ¥":
                    match = re.match(r"^(\d+)(.*)$", val)
                    if match: val = f"{match.group(2)}(æŒ‡æ•¸{match.group(1)})"
                vals.append(val)
            parsed_data[row_name] = vals
            
        results = []
        for i in range(len(dates)):
            item = {"location": location_name, "pid": pid, "date": dates[i], "æ™‚é–“": "ç™½å¤©" if i % 2 == 0 else "æ™šä¸Š"}
            for k, v in parsed_data.items(): 
                if k == "æ™‚é–“": continue
                item[k] = v[i] if i < len(v) else "æœªçŸ¥"
            results.append(item)
        return results
    except Exception as e:
        print(f"âŒ çˆ¬å–éŒ¯èª¤ ({location_name}): {e}")
        return []

# æ›´æ–° CSV æª”æ¡ˆ (å¯æ’ç¨‹åŸ·è¡Œ)
def update_weekly_csv():
    file_name = "all_taiwan_star_forecast.csv"
    print(f"ğŸš€ é–‹å§‹æ›´æ–°æ¯é€±é å ±è³‡æ–™ (å…± {len(all_locations)} è™•)...")
    final_data = []
    for pid, name in all_locations.items():
        data = scrape_weekly_table(pid, name)
        if data: final_data.extend(data)
        time.sleep(0.2)
    
    if final_data:
        new_df = pd.DataFrame(final_data)
        if os.path.exists(file_name) and os.path.getsize(file_name) > 0:
            try:
                final_df = pd.concat([pd.read_csv(file_name, encoding="utf-8-sig"), new_df], ignore_index=True).drop_duplicates(subset=['location', 'date', 'æ™‚é–“'], keep='last')
            except: final_df = new_df
        else: final_df = new_df
        final_df.to_csv(file_name, index=False, encoding="utf-8-sig")
        print(f"âœ… CSV æ›´æ–°å®Œæˆï¼ç›®å‰å…±æœ‰ {len(final_df)} ç­†æ•¸æ“šã€‚")

# æŸ¥è©¢å‡½å¼ (å›å‚³å­—ä¸² - æ ¼å¼å·²èª¿æ•´)
def get_weekly_star_info(user_input):
    file_name = "all_taiwan_star_forecast.csv"
    try:
        if not os.path.exists(file_name): return "âš ï¸ æ‰¾ä¸åˆ°è³‡æ–™æª”ï¼Œè«‹è¯ç¹«ç®¡ç†å“¡æ›´æ–°è³‡æ–™åº«ã€‚"
        df = pd.read_csv(file_name, encoding="utf-8-sig")
        # åƒ…ç¯©é¸æ™šä¸Š
        target_df = df[(df['location'].str.contains(user_input)) & (df['æ™‚é–“'] == "æ™šä¸Š")].copy()
        
        if target_df.empty: return f"æ‰¾ä¸åˆ°ã€Œ{user_input}ã€çš„é å ±è³‡æ–™ã€‚"

        tz_taiwan = timezone(timedelta(hours=8))
        today = datetime.now(tz_taiwan).date()
        this_year = datetime.now(tz_taiwan).year

        def is_future(date_str):
            try: return datetime.strptime(f"{this_year}/{date_str[:5]}", "%Y/%m/%d").date() >= today
            except: return True
            
        target_df = target_df[target_df['date'].apply(is_future)]
        data_list = target_df.head(7).to_dict('records')
        
        if not data_list: return f"ç›®å‰æ²’æœ‰ {user_input} æœªä¾†ä¸€é€±çš„è³‡æ–™ã€‚"

        all_blocks = []
        for item in data_list:
            score = 1
            weather = item.get('å¤©æ°£ç‹€æ³', 'æœªçŸ¥')
            
            # --- åˆ†æ•¸è¨ˆç®— ---
            if "æ™´" in weather: score += 2
            try:
                if int(item.get('æœ€é«˜æº«', 0)) > 15: score += 1
                if 20 <= int(item.get('æœ€é«˜æº«', 0)) <= 25: score += 1
            except: pass
            try:
                if int(re.findall(r'\d+', str(item.get('è’²ç¦é¢¨ç´š', '0')))[-1]) >= 5: score -= 1
            except: pass
            stars = "â­" * max(1, min(5, score))

            # --- ç¶œåˆè©•ä¼° ---
            if "æ™´" not in weather:
                eval_msg = "ä»Šæ™šä¸é©åˆè§€æ˜Ÿã€‚"
            else:
                try:
                    fl = int(item.get('é«”æ„Ÿæœ€ä½æº«', 20))
                    if fl < 15: eval_msg = "å¤©æ°£å¯’å†·ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°å¤šç©¿å¹¾ä»¶ä¿æš–è¡£ç‰©ï¼"
                    elif 15 <= fl < 20: eval_msg = "å¤©æ°£ç¨æ¶¼ï¼Œå¤–å‡ºè§€æ˜Ÿå»ºè­°ç©¿ä»¶è–„å¤–å¥—ï¼"
                    elif 20 <= fl <= 25: eval_msg = "å¤©æ°£èˆ’é©ï¼Œçµ•ä½³è§€æ˜Ÿæ—¥ï¼"
                    else: eval_msg = "é©åˆè§€æ˜Ÿçš„æº«ç†±å¤œæ™šï¼"
                except: eval_msg = "è«‹æ³¨æ„ç¾å ´å¤©æ°£è®ŠåŒ–ã€‚"
                
                # ç–ŠåŠ é¢¨åŠ›è­¦ç¤º
                try:
                    if int(re.findall(r'\d+', str(item.get('è’²ç¦é¢¨ç´š', '0')))[-1]) >= 5:
                        eval_msg += " (å¦å¤–ä»Šæ™šé¢¨åŠ›è¼ƒå¼·ï¼Œè¡Œç¶“è¦–ç·šæ˜æš—è™•è«‹å°å¿ƒï¼)"
                except: pass
                
                

            # --- æ ¼å¼çµ„è£ (ç›´åˆ—å¼) ---
            res = [
                f"ğŸ“… {item['date']} ({item['æ™‚é–“']})",
                f"å¤©æ°£ï¼š{weather}",
                f"æ°£æº«ï¼š{item.get('æœ€ä½æº«', '?')}~{item.get('æœ€é«˜æº«', '?')}Â°C",
                f"é«”æ„Ÿï¼š{item.get('é«”æ„Ÿæœ€ä½æº«', '?')}~{item.get('é«”æ„Ÿæœ€é«˜æº«', '?')}Â°C",
                f"é™é›¨ï¼š{item.get('é™é›¨æ©Ÿç‡', 'æœªçŸ¥')}",
                f"è§€æ˜Ÿæ¨è–¦æŒ‡æ•¸ï¼š{stars}",
                f"ğŸ“ ç¶œåˆè©•ä¼°ï¼š{eval_msg}"
            ]
            all_blocks.append("\n".join(res))
            
        header = f"ğŸŒŒ ã€{user_input}ã€‘æœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å—\n\n"
        tail = "\n\n----------------\nğŸ”” æº«é¦¨æé†’ï¼šç•¶æ—¥å¯å†ç¢ºèªæ™´æœ—çš„æ™šé–“æ™‚æ®µå“¦~\n\n"
        return header + "\n\n----------------\n".join(all_blocks) + tail
    except Exception as e: return f"âŒ éŒ¯èª¤ï¼š{str(e)}"


# ==========================================
# åŠŸèƒ½ Bï¼šè‡¨æ™‚èˆˆèµ· (72hr å³æ™‚çˆ¬èŸ²)
# ==========================================

def format_time_ranges(time_list):
    if not time_list: return ""
    hours = [int(t.split(':')[0]) for t in time_list]
    processed = [h + 24 if h <= 5 and any(p >= 18 for p in hours) else h for h in hours]
    
    ranges = []
    start_h = prev_h = processed[0]
    for i in range(1, len(processed)):
        curr = processed[i]
        if curr == prev_h + 1: prev_h = curr
        else:
            ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
            start_h = prev_h = curr
    ranges.append(f"{start_h%24:02d}:00-{(prev_h+1)%24:02d}:00")
    return "ã€".join(ranges)

# æŸ¥è©¢å‡½å¼ (å³æ™‚çˆ¬èŸ²)
def get_impromptu_star_info(pid, location_name):
    url = f"https://www.cwa.gov.tw/V8/C/L/StarView/MOD/3hr/{pid}_3hr_PC.html"
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.encoding = 'utf-8'
        raw_html = f"<table>{resp.text}</table>" if "<table" not in resp.text else resp.text
        soup = BeautifulSoup(raw_html, "html.parser")
        
        date_map = {th.get("id").split("_")[-1]: th.get_text(strip=True)[:5] 
                    for th in soup.find_all("th") if th.get("id") and "PC3_D" in th.get("id") and "H" not in th.get("id") and th.get("id") != "PC3_D"}
        time_row = soup.find("tr", class_="time")
        time_ids = [th.get('id') for th in time_row.find_all("th")[1:] if th.get('id')]
        time_full_labels = {tid: f"{date_map[next(dk for dk in date_map if dk in tid)]} {time_row.find('th', id=tid).get_text(strip=True)}" for tid in time_ids}

        master_data = {tid: {} for tid in time_ids}
        for row in soup.find_all("tr"):
            th = row.find("th")
            if not th or "æ™‚é–“" in th.get_text(): continue
            title = th.get_text(strip=True)
            for td in row.find_all("td"):
                h_attr = td.get('headers', "")
                val = td.find("img").get('title') if td.find("img") else (td.find("span", class_="tem-C").get_text(strip=True) if td.find("span", class_="tem-C") else td.get_text(strip=True))
                for tid in time_ids:
                    if tid in h_attr: master_data[tid][title] = val

        perfect_times, cloudy_times = [], []
        for tid in time_ids[:24]:
            time_str = time_full_labels[tid].split(" ")[1]
            hour = int(time_str.split(":")[0])
            if hour >= 18 or hour <= 5:
                w = master_data[tid].get("å¤©æ°£ç‹€æ³", "æœªçŸ¥")
                if "æ™´" in w: perfect_times.append(time_str)
                elif "å¤šé›²" in w: cloudy_times.append(time_str)

        if perfect_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜Š \nå¤ªæ£’äº†ï¼Œä»Šæ™šæœ€é©åˆè§€æ˜Ÿçš„æ™‚æ®µç‚ºï¼š{format_time_ranges(perfect_times)}"
        elif cloudy_times:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜ \nä»Šæ™šé›²é‡è¼ƒå¤šï¼Œå¯ç¢°é‹æ°£çš„æ™‚æ®µç‚ºï¼š{format_time_ranges(cloudy_times)}"
        else:
            return f"ğŸ”­ ã€{location_name}ã€‘è§€æ˜Ÿå»ºè­°ï¼šğŸ˜­ \nä»Šæ™šä¸é©åˆè§€æ˜Ÿï¼Œè«‹å¥½å¥½ç¡è¦ºã€‚"
    except Exception as e:
        return f"âŒ ç³»çµ±éŒ¯èª¤: {str(e)}"

# ==========================================
# ä¸»ç¨‹å¼æ¸¬è©¦å€
# ==========================================
if __name__ == "__main__":
    # 1. ç¬¬ä¸€æ¬¡åŸ·è¡Œå»ºè­°è·‘ä¸€æ¬¡æ›´æ–°ï¼Œä¹‹å¾Œå¯è¨»è§£æ‰
    update_weekly_csv() 
    
    print("\n--------- æ¨¡æ“¬ LINE Bot ä½¿ç”¨è€…æ“ä½œ ---------")
    
    # ä½¿ç”¨è€…é¸æ“‡æƒ…å¢ƒ Aï¼šè¦åŠƒæœªä¾†
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šæœªä¾†ä¸€é€±è§€æ˜ŸæŒ‡å— -> é¸æ“‡ï¼šé™½æ˜å±±å°æ²¹å‘")
    print(get_weekly_star_info("å°æ²¹å‘"))
    
    print("\n-------------------------------------------")
    
    # ä½¿ç”¨è€…é¸æ“‡æƒ…å¢ƒ Bï¼šè‡¨æ™‚å‡ºç™¼
    print("ğŸ”¹ ç”¨æˆ¶é»é¸ï¼šè‡¨æ™‚èˆˆèµ·å»è§€æ˜Ÿ -> é¸æ“‡ï¼šé¹¿æ—å¤©æ–‡å°")
    # æ³¨æ„ï¼šé€™è£¡éœ€è¦å‚³å…¥ PID (F017)
    print(get_impromptu_star_info("F017", "é¹¿æ—å¤©æ–‡å°"))